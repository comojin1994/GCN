{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers import GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../Datasets/cora/'\n",
    "SEED = 42\n",
    "\n",
    "channels = 16\n",
    "dropout = 0.5\n",
    "l2_reg = 5e-4\n",
    "learning_rate = 1e-2\n",
    "epochs = 200\n",
    "es_patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cora Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_edges = []\n",
    "\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if '.content' in file:\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                all_data.extend(f.read().splitlines())\n",
    "        elif '.cites' in file:\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                all_edges.extend(f.read().splitlines())\n",
    "\n",
    "all_data = shuffle(all_data, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cora.content\n",
    "1. Node ID\n",
    "2. Node Feature\n",
    "3. Node Label\n",
    "\n",
    "cora.cites\n",
    "\n",
    "(a, b)\n",
    "\n",
    "a : ID of the paper being cited\n",
    "\n",
    "b : the paper containing the citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG >>> X shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "nodes = []\n",
    "X = []\n",
    "\n",
    "for i, data in enumerate(all_data):\n",
    "    elements = data.split('\\t')\n",
    "    labels.append(elements[-1])\n",
    "    X.append(elements[1:-1])\n",
    "    nodes.append(elements[0])\n",
    "    \n",
    "X = np.array(X, dtype=int)\n",
    "N = X.shape[0]\n",
    "F = X.shape[1]\n",
    "print(f'LOG >>> X shape: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG >>> Number of Nodes N : 2708\n",
      "LOG >>> Number of Features F of each node : 1433\n",
      "LOG >>> Categories :\n",
      "{'Rule_Learning', 'Reinforcement_Learning', 'Theory', 'Neural_Networks', 'Case_Based', 'Probabilistic_Methods', 'Genetic_Algorithms'}\n",
      "LOG >>> Number of classes : 7\n"
     ]
    }
   ],
   "source": [
    "edge_list = []\n",
    "\n",
    "for edge in all_edges:\n",
    "    e = edge.split('\\t')\n",
    "    edge_list.append((e[0], e[1]))\n",
    "    \n",
    "print(f'LOG >>> Number of Nodes N : {N}')\n",
    "print(f'LOG >>> Number of Features F of each node : {F}')\n",
    "print(f'LOG >>> Categories :\\n{set(labels)}')\n",
    "num_classes = len(set(labels))\n",
    "print(f'LOG >>> Number of classes : {num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Train, val and test mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_data(labels, limit=20, val_num=500, test_num=1000):\n",
    "    label_counter = dict((l, 0) for l in labels)\n",
    "    train_idx = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        if label_counter[label] < limit:\n",
    "            train_idx.append(i)\n",
    "            label_counter[label] += 1\n",
    "        \n",
    "        if all(count == limit for count in label_counter.values()):\n",
    "            break\n",
    "            \n",
    "    rest_idx = [x for x in range(len(labels)) if x not in train_idx]\n",
    "    val_idx = rest_idx[:val_num]\n",
    "    test_idx = rest_idx[val_num:(val_num + test_num)]\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = limit_data(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG >>> Number of Train set : 140\n",
      "LOG >>> Number of Valid set : 500\n",
      "LOG >>> Number of Test set : 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'LOG >>> Number of Train set : {len(train_idx)}')\n",
    "print(f'LOG >>> Number of Valid set : {len(val_idx)}')\n",
    "print(f'LOG >>> Number of Test set : {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.zeros((N,), dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "\n",
    "val_mask = np.zeros((N,), dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "\n",
    "test_mask = np.zeros((N,), dtype=bool)\n",
    "test_mask[test_idx] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Adjacency Matrix, A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG >>> Graph info:\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 2708\n",
      "Number of edges: 5278\n",
      "Average degree:   3.8981\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "A = nx.adjacency_matrix(G)\n",
    "print(f'LOG >>> Graph info:\\n{nx.info(G)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(30, 30))\n",
    "# nx.draw(G, with_labels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode label with One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "\n",
    "labels_encoded, classes = encode_label(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph and get A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG >>> Graph info:\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 2708\n",
      "Number of edges: 5278\n",
      "Average degree:   3.8981\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "# A = nx.adjacency_matrix(G).todense()\n",
    "A = nx.adjacency_matrix(G)\n",
    "print(f'LOG >>> Graph info:\\n{nx.info(G)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Adj(A):\n",
    "    I = np.identity(A.shape[0])\n",
    "    A_hat = A + I\n",
    "    D = np.diag(np.squeeze(np.array(np.sum(A_hat, axis=0))))\n",
    "    D_half_norm = fractional_matrix_power(D, -0.5)\n",
    "    DAD = D_half_norm.dot(A_hat).dot(D_half_norm)\n",
    "    return DAD\n",
    "\n",
    "A = normalize_Adj(A)\n",
    "X = X.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(tf.keras.Model):\n",
    "    def __init__(self, filters, dropout):\n",
    "        super(GCN, self).__init__(name='GCN')\n",
    "        self.dropout_1 = Dropout(dropout)\n",
    "        self.graphConv_1 = GraphConv(filters,\n",
    "                                    activation=tf.nn.relu,\n",
    "                                    use_bias=False)\n",
    "        self.dropout_2 = Dropout(dropout)\n",
    "        self.graphConv_2 = GraphConv(filters,\n",
    "                                    activation=tf.nn.softmax,\n",
    "                                    use_bias=False)\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        A, x = input_tensor\n",
    "        x = self.dropout_1(x)\n",
    "        A, x = self.graphConv_1([A, x])\n",
    "        x = self.dropout_2(x)\n",
    "        A, x = self.graphConv_2([A, x])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(channels, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             weighted_metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2708, 16), dtype=float32, numpy=\n",
       "array([[0.06288707, 0.06121243, 0.06058193, ..., 0.06163995, 0.05895111,\n",
       "        0.0621356 ],\n",
       "       [0.05935992, 0.06203778, 0.05843735, ..., 0.06627774, 0.06009815,\n",
       "        0.060916  ],\n",
       "       [0.0616232 , 0.06467518, 0.05866429, ..., 0.06264294, 0.05901151,\n",
       "        0.06408861],\n",
       "       ...,\n",
       "       [0.06246663, 0.06287847, 0.05902089, ..., 0.06126496, 0.06226303,\n",
       "        0.0635891 ],\n",
       "       [0.05774948, 0.06767119, 0.05999758, ..., 0.06067945, 0.05742813,\n",
       "        0.06666148],\n",
       "       [0.05986262, 0.06507587, 0.06025335, ..., 0.0646367 , 0.05830977,\n",
       "        0.0638813 ]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([A, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCN",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
